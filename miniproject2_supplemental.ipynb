{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "miniproject2_supplemental.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HungYangChang/ECSE-551-Mini-project2/blob/main/miniproject2_supplemental.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNYWZ6a6Rwhd"
      },
      "source": [
        "# Mini-Project 2 Supplemental Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkCGwrtgR3OC"
      },
      "source": [
        "# Import relevant modules\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.feature_extraction import text\n",
        "from sklearn import model_selection\n",
        "from sklearn import svm\n",
        "import time\n",
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBSTi2dGSBSq"
      },
      "source": [
        "# Load and Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_hgSflFSE9z"
      },
      "source": [
        "# Load testing and training data\n",
        "url = \"https://raw.githubusercontent.com/jonarsenault/ecse551data/master/train.csv\"\n",
        "train_data = pd.read_csv(url)\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/jonarsenault/ecse551data/master/test.csv\"\n",
        "test_data = pd.read_csv(url)\n",
        "\n",
        "# Parameters\n",
        "number_of_samples = None  # Set to None to test entire data set\n",
        "stop_words = text.ENGLISH_STOP_WORDS\n",
        "np.random.seed(10)\n",
        "\n",
        "# For some reason, need to shuffle even if using all data\n",
        "if number_of_samples is None:\n",
        "    number_of_samples = len(train_data)\n",
        "\n",
        "train_data = train_data.sample(number_of_samples).reset_index(drop=True)\n",
        "\n",
        "X = train_data[\"body\"]\n",
        "y = train_data[\"subreddit\"]\n",
        "\n",
        "X_test = test_data[\"body\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPKosZCqVZey"
      },
      "source": [
        "# Parameters\n",
        "\n",
        "# Add some stop words\n",
        "more_stop_words = [\n",
        "    \"u\",\n",
        "    \"just\",\n",
        "    \"think\",\n",
        "    \"https\",\n",
        "    \"www\",\n",
        "    \"don't\",\n",
        "    \"like\",\n",
        "    \"need\",\n",
        "    \"it\",\n",
        "    \"you're\",\n",
        "    \"use\",\n",
        "    \"reddit\",\n",
        "    \"thing\",\n",
        "    \"I'm\",\n",
        "    \"things\",\n",
        "    \"good\",\n",
        "    \"really\",\n",
        "    \"want\",\n",
        "    \"maybe\",\n",
        "    \"imgur\",\n",
        "    \"com\",\n",
        "    \"don\",\n",
        "    \"actually\",\n",
        "    \"that\",\n",
        "    \"make\",\n",
        "    \"lot\",\n",
        "    \"different\",\n",
        "    \"doing\",\n",
        "    \"that\",\n",
        "    \"better\",\n",
        "    \"going\",\n",
        "    \"great\",\n",
        "]\n",
        "\n",
        "fewer_stop_words = [\n",
        "    \"u\",\n",
        "    \"just\",\n",
        "    \"think\",\n",
        "    \"don't\",\n",
        "    \"like\",\n",
        "    \"need\",\n",
        "    \"it\",\n",
        "    \"you're\",\n",
        "    \"use\",\n",
        "    \"thing\",\n",
        "    \"I'm\",\n",
        "    \"things\",\n",
        "    \"good\",\n",
        "    \"really\",\n",
        "    \"want\",\n",
        "    \"maybe\",\n",
        "    \"don\",\n",
        "    \"actually\",\n",
        "    \"that\",\n",
        "    \"make\",\n",
        "    \"lot\",\n",
        "    \"different\",\n",
        "    \"doing\",\n",
        "    \"that\",\n",
        "    \"better\",\n",
        "    \"going\",\n",
        "    \"great\",\n",
        "]\n",
        "almost_all_stop_words = stop_words.union(fewer_stop_words)\n",
        "all_stop_words = stop_words.union(more_stop_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-wACgE9SRlq"
      },
      "source": [
        "# Define Naive Bayes class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTd7foSlSszy"
      },
      "source": [
        "\n",
        "class NaiveBayes:\n",
        "    def __init__(self, alpha=0.01, prior=\"learn\"):\n",
        "        \"\"\"Constructor\"\"\"\n",
        "\n",
        "        self.alpha = alpha\n",
        "        self.prior = prior\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Obtain naive bayes parameters from training data. X is input data, \n",
        "        y are class labels\"\"\"\n",
        "\n",
        "        # Convert sparse array to dense array\n",
        "        X = X.toarray()\n",
        "\n",
        "        # Compute each class probability\n",
        "        class_counts = y.value_counts()\n",
        "\n",
        "        num_labels = len(class_counts)\n",
        "\n",
        "        if self.prior == \"learn\":\n",
        "          # Learn the class probabilities from the training data\n",
        "          self.class_probabilities = class_counts / len(y)\n",
        "        elif self.prior == \"uniform\":\n",
        "          # Assume a uniform prior\n",
        "          self.class_probabilities = pd.Series(np.repeat(1/num_labels, num_labels), \n",
        "                                               index = class_counts.index)\n",
        "\n",
        "        # Sort in alphabetical order\n",
        "        self.class_probabilities.sort_index(inplace=True)\n",
        "        class_counts.sort_index(inplace=True)\n",
        "\n",
        "        # Compute parameters\n",
        "        features_count = np.empty((num_labels, X.shape[1]))\n",
        "\n",
        "        y_numpy = y.to_numpy()\n",
        "        for i in range(num_labels):\n",
        "\n",
        "            label = self.class_probabilities.index[i]\n",
        "            X_this_label = X[np.nonzero(y_numpy == label), :]\n",
        "\n",
        "            features_count[i,:] = np.sum(X_this_label, axis=1)\n",
        "\n",
        "        # Laplace smoothing\n",
        "        smoothed_numerator = features_count + self.alpha\n",
        "        smoothed_denominator = np.sum(smoothed_numerator,axis=1).reshape(-1,1)\n",
        "\n",
        "        self.parameters = pd.DataFrame(smoothed_numerator / smoothed_denominator, index=self.class_probabilities.index)\n",
        "        \n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict class of text\"\"\"\n",
        "\n",
        "        X = X.toarray()\n",
        "\n",
        "        delta = pd.DataFrame(columns=self.class_probabilities.index)\n",
        "        for label in self.class_probabilities.index:\n",
        "\n",
        "            # Get probability of currect class P(y=k)\n",
        "            class_probability = self.class_probabilities[label]\n",
        "\n",
        "            # Get theta_j for currect class\n",
        "            theta_j_class = self.parameters.loc[label, :].to_numpy()\n",
        "\n",
        "            # Compute P(x_j | y = k)\n",
        "            prob_features_given_y = (theta_j_class ** X) * (1 - theta_j_class) ** (\n",
        "                1 - X\n",
        "            )\n",
        "\n",
        "            # Compute P(x | y = k)\n",
        "            prob_sample_given_y = np.prod(prob_features_given_y, axis=1)\n",
        "\n",
        "            # Compute P(y) * P(x | y = k)\n",
        "            term1 = np.log(class_probability)\n",
        "            term2 = np.sum(X * np.log(theta_j_class), axis=1)\n",
        "            term3 = np.sum((1 - X) * np.log(1 - theta_j_class), axis=1)\n",
        "            delta_k = term1 + term2 + term3\n",
        "\n",
        "            # Append\n",
        "            delta[label] = delta_k\n",
        "\n",
        "        predicted_class = delta.idxmax(axis=1)\n",
        "\n",
        "        return predicted_class.to_list()\n",
        "\n",
        "    def score(self, X, y):\n",
        "        \"\"\"Compute accuracy of naive bayes model\"\"\"      \n",
        "\n",
        "        y_pred = self.predict(X)\n",
        "\n",
        "        accuracy = np.count_nonzero(y == y_pred) / len(y_pred)\n",
        "\n",
        "        return accuracy\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        \"\"\"Getter for parameters\"\"\"\n",
        "\n",
        "        params = {\"alpha\": self.alpha,\n",
        "                  \"prior\": self.prior}\n",
        "\n",
        "        return params\n",
        "\n",
        "    def set_params(self, **parameters):\n",
        "        \"Setter for parameters\"\n",
        "        for parameter, value in parameters.items():\n",
        "            setattr(self, parameter, value)\n",
        "\n",
        "        return self\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzBD525E6gQ_"
      },
      "source": [
        "# Choosing parameters and preprocessing methods for Multinomial Naive Bayes model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlMlSCGK6gRA"
      },
      "source": [
        "#Define functions of lemmatization and stemming\n",
        "\n",
        "from nltk import word_tokenize          \n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer \n",
        "\n",
        "class LemmaTokenizer_1:\n",
        "     def __init__(self):\n",
        "       self.wnl = WordNetLemmatizer()\n",
        "     def __call__(self, doc):\n",
        "       return [self.wnl.lemmatize(t,pos =\"v\") for t in word_tokenize(doc)]\n",
        "\n",
        "class LemmaTokenizer_2:\n",
        "     def __init__(self):\n",
        "       self.wnl = WordNetLemmatizer()\n",
        "     def __call__(self, doc):\n",
        "       return [self.wnl.lemmatize(t,pos =\"v\") for t in word_tokenize(doc) if t.isalpha()]\n",
        "\n",
        "class StemTokenizer:\n",
        "     def __init__(self):\n",
        "       self.wnl =PorterStemmer()\n",
        "     def __call__(self, doc):\n",
        "       return [self.wnl.stem(t) for t in word_tokenize(doc) if t.isalpha()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjpbGd0-FN5N"
      },
      "source": [
        "## No preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2e3o3oh6gRD",
        "outputId": "f9361baa-0fda-4f82-b955-e0c96d5c1033",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "## Test with no preprocessing\n",
        "\n",
        "t_start = time.time()\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "naive_bayes_model = NaiveBayes(alpha = 0.01, prior=\"uniform\")\n",
        "\n",
        "pipe = Pipeline(\n",
        "    [(\"vect\", vectorizer),  (\"classify\", naive_bayes_model)]\n",
        ")\n",
        "\n",
        "cross_val_accuracy = model_selection.cross_val_score(pipe, X, y, n_jobs=-1)\n",
        "\n",
        "t_end = time.time()\n",
        "\n",
        "print(f\"The 5-fold cross-validation accuracy is: {np.mean(cross_val_accuracy):.5f}\")\n",
        "print(f\"Run time: {t_end-t_start: .3f} seconds\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The 5-fold cross-validation accuracy is: 0.89449\n",
            "Run time:  119.945 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xI8B03BDFXv7"
      },
      "source": [
        "## Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oy-oEySY6gRI",
        "outputId": "bd46eebd-8ea5-4d2c-d12f-0f8bf0d067d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "## Test: normalize\n",
        "\n",
        "t_start = time.time()\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "normalizer = Normalizer()\n",
        "naive_bayes_model = NaiveBayes(alpha = 0.01, prior=\"uniform\")\n",
        "\n",
        "pipe = Pipeline(\n",
        "    [(\"vect\", vectorizer), (\"norm\", normalizer),  (\"classify\", naive_bayes_model)]\n",
        ")\n",
        "\n",
        "cross_val_accuracy = model_selection.cross_val_score(pipe, X, y, n_jobs=-1)\n",
        "\n",
        "t_end = time.time()\n",
        "\n",
        "print(f\"The 5-fold cross-validation accuracy is: {np.mean(cross_val_accuracy):.5f}\")\n",
        "print(f\"Run time: {t_end-t_start: .3f} seconds\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The 5-fold cross-validation accuracy is: 0.90563\n",
            "Run time:  100.661 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k__uW_ohFcXS"
      },
      "source": [
        "## Remove stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Qjb3-OP6gRL",
        "outputId": "4ac06142-ff7c-4435-9ad3-dd5ac6c64d7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "##Test: remove stop-words\n",
        "\n",
        "t_start = time.time()\n",
        "\n",
        "vectorizer = CountVectorizer(stop_words=all_stop_words)\n",
        "normalizer = Normalizer()\n",
        "naive_bayes_model = NaiveBayes(alpha = 0.01, prior=\"uniform\")\n",
        "\n",
        "pipe = Pipeline(\n",
        "    [(\"vect\", vectorizer), (\"norm\", normalizer),  (\"classify\", naive_bayes_model)]\n",
        ")\n",
        "\n",
        "cross_val_accuracy = model_selection.cross_val_score(pipe, X, y, n_jobs=-1)\n",
        "\n",
        "t_end = time.time()\n",
        "\n",
        "print(f\"The 5-fold cross-validation accuracy is: {np.mean(cross_val_accuracy):.5f}\")\n",
        "print(f\"Run time: {t_end-t_start: .3f} seconds\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The 5-fold cross-validation accuracy is: 0.90658\n",
            "Run time:  100.708 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNb7h7xpFgc4"
      },
      "source": [
        "## Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoTZr6kZBiJJ",
        "outputId": "0e1526c8-9b91-450a-da65-24e8f24a8596",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "##Test: lemmatization that works on colab\n",
        "\n",
        "t_start = time.time()\n",
        "\n",
        "from nltk import word_tokenize          \n",
        "from nltk.stem import WordNetLemmatizer \n",
        "class LemmaTokenizer:\n",
        "     def __init__(self):\n",
        "       self.wnl = WordNetLemmatizer()\n",
        "     def __call__(self, doc):\n",
        "       return [self.wnl.lemmatize(t,pos =\"v\") for t in word_tokenize(doc) if t.isalpha()]\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "X1 = X.copy()\n",
        "\n",
        "for i in range(X1.shape[0]):\n",
        "    X1[i] = X1[i].lower()\n",
        "    X1[i] = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(X1[i])]\n",
        "    X1[i] = TreebankWordDetokenizer().detokenize(X1[i]) #detokenize\n",
        "print(X1[0]) \n",
        "#Although it was defined in lemmarizer function to remove isalpha, this print shows that it doesn't remove all non-alphabetic characters. \n",
        "#A second step is taken to ensure is alpha in the next test \n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "normalizer = Normalizer()\n",
        "naive_bayes_model = NaiveBayes(alpha = 0.01, prior=\"uniform\")\n",
        "\n",
        "pipe = Pipeline(\n",
        "    [(\"vect\", vectorizer), (\"norm\", normalizer),  (\"classify\", naive_bayes_model)]\n",
        ")\n",
        "\n",
        "cross_val_accuracy = model_selection.cross_val_score(pipe, X1, y, n_jobs=-1)\n",
        "\n",
        "t_end = time.time()\n",
        "\n",
        "print(f\"The 5-fold cross-validation accuracy is: {np.mean(cross_val_accuracy):.5f}\")\n",
        "print(f\"Run time: {t_end-t_start: .3f} seconds\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "**the follow be the deal highlight: ** * **price: ** $40b nvidia stock and cash * **accretion: ** immediately accretive to nvidia non-gaap gross margin and eps * **cambridge investment: ** create “ world-class ” ai research and education center for healthcare life science robotics and self-driving car . also build an arm/nvidia-based ai supercomputer for research * **softbank ownership: ** will keep 10% stake in new entity **the follow be the operating highlight: ** * **arm operating structure: ** arm will operate a an nvidia division * **arm locality: ** arm will continue to be headquarter in cambridge * **ip locality: ** will keep registration in the uk * **licensing model: ** continue to operate it open-licensing model while maintain it global customer neutrality **new opportunities** *the new nvidia-arm combination now play in nearly every market segment in the datacenter edge of datacenter personal computer smartphones and the iot . my imagination run wild with the possibility of: * * more “ big-core ” arm datacenter general-purpose processor * big cpu gpu npu and networking datacenter combination * cpu-gpu-npu combination with share memory system for the hpc market * nvidia-based smartphone and tablet gpu/npu ip * arm-based soc for window notebook * arm-based big core cpu for highest-performance window desktop\n",
            "The 5-fold cross-validation accuracy is: 0.90589\n",
            "Run time:  251.877 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eCuzcgVFmR3"
      },
      "source": [
        "## Lemmatization and remove non-alphabetic characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ne8MED1BDQ3g",
        "outputId": "190c8051-f963-48ba-bd5b-4e9aa0d5b768",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Tests: lemmatization with isalpha that works on colab\n",
        "\n",
        "t_start = time.time()\n",
        "\n",
        "X1 = X.copy()\n",
        "for i in range(X1.shape[0]):\n",
        "    X1[i] = X1[i].lower()\n",
        "    X1[i] = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(X1[i])]\n",
        "    X1[i] = [w for w in X1[i] if w.isalpha()] #Remove non-alphabetic words\n",
        "    X1[i] = TreebankWordDetokenizer().detokenize(X1[i]) #detokenize\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "normalizer = Normalizer()\n",
        "naive_bayes_model = NaiveBayes(alpha = 0.01, prior=\"uniform\")\n",
        "\n",
        "pipe = Pipeline(\n",
        "    [(\"vect\", vectorizer), (\"norm\", normalizer),  (\"classify\", naive_bayes_model)]\n",
        ")\n",
        "\n",
        "cross_val_accuracy = model_selection.cross_val_score(pipe, X1, y, n_jobs=-1)\n",
        "\n",
        "t_end = time.time()\n",
        "\n",
        "print(f\"The 5-fold cross-validation accuracy is: {np.mean(cross_val_accuracy):.5f}\")\n",
        "print(f\"Run time: {t_end-t_start: .3f} seconds\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The 5-fold cross-validation accuracy is: 0.89864\n",
            "Run time:  242.344 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_h9RGOcFs-f"
      },
      "source": [
        "## ngram (1,2) (1,3) (1,4) with max_features=40000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9JUZV2u6gRR",
        "outputId": "b7ca7685-fa97-4018-8be5-adec9b632619",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "##Tests: ngram (1,2)\n",
        "\n",
        "t_start = time.time()\n",
        "\n",
        "vectorizer = CountVectorizer(ngram_range=(1, 2), max_features = 40000)\n",
        "normalizer = Normalizer()\n",
        "naive_bayes_model = NaiveBayes(alpha = 0.01, prior=\"uniform\")\n",
        "\n",
        "pipe = Pipeline(\n",
        "    [(\"vect\", vectorizer), (\"norm\", normalizer), (\"classify\", naive_bayes_model)]\n",
        ")\n",
        "\n",
        "cross_val_accuracy = model_selection.cross_val_score(pipe, X, y, n_jobs=-1)\n",
        "\n",
        "t_end = time.time()\n",
        "\n",
        "print(f\"The 5-fold cross-validation accuracy is: {np.mean(cross_val_accuracy):.5f}\")\n",
        "print(f\"Run time: {t_end-t_start: .3f} seconds\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The 5-fold cross-validation accuracy is: 0.88836\n",
            "Run time:  127.162 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3mXz4jI6gRT",
        "outputId": "cfd43b49-2d57-420e-a075-2fa08ca26155",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "##Tests: ngram (1,3)\n",
        "\n",
        "t_start = time.time()\n",
        "\n",
        "vectorizer = CountVectorizer(ngram_range=(1,3), max_features = 40000)\n",
        "normalizer = Normalizer()\n",
        "naive_bayes_model = NaiveBayes(alpha = 0.01, prior=\"uniform\")\n",
        "\n",
        "pipe = Pipeline(\n",
        "    [(\"vect\", vectorizer), (\"norm\", normalizer),  (\"classify\", naive_bayes_model)]\n",
        ")\n",
        "\n",
        "cross_val_accuracy = model_selection.cross_val_score(pipe, X, y, n_jobs=-1)\n",
        "\n",
        "t_end = time.time()\n",
        "\n",
        "print(f\"The 5-fold cross-validation accuracy is: {np.mean(cross_val_accuracy):.5f}\")\n",
        "print(f\"Run time: {t_end-t_start: .3f} seconds\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The 5-fold cross-validation accuracy is: 0.87265\n",
            "Run time:  154.061 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKWwRHu26gRV",
        "outputId": "b51c6918-5341-430e-a205-43e7a58238e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "##Tests: ngram (1,4)\n",
        "\n",
        "t_start = time.time()\n",
        "\n",
        "vectorizer = CountVectorizer(ngram_range=(1,4), max_features = 40000)\n",
        "normalizer = Normalizer()\n",
        "naive_bayes_model = NaiveBayes(alpha = 0.01, prior=\"uniform\")\n",
        "\n",
        "pipe = Pipeline(\n",
        "    [(\"vect\", vectorizer), (\"norm\", normalizer),  (\"classify\", naive_bayes_model)]\n",
        ")\n",
        "\n",
        "cross_val_accuracy = model_selection.cross_val_score(pipe, X, y, n_jobs=-1)\n",
        "\n",
        "t_end = time.time()\n",
        "\n",
        "print(f\"The 5-fold cross-validation accuracy is: {np.mean(cross_val_accuracy):.5f}\")\n",
        "print(f\"Run time: {t_end-t_start: .3f} seconds\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The 5-fold cross-validation accuracy is: 0.86539\n",
            "Run time:  166.375 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJ6G8yxCNRrS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}